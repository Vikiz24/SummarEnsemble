{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbU-G3qhvE_K",
        "outputId": "9c3d00c2-93a6-4fad-da3c-cfc7b97bc5eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# Connecting to Google Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import re\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
      ],
      "metadata": {
        "id": "cS26LjRPwJMR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the T5 model\n",
        "drive_path_to_load_t5_model = 'gdrive/My Drive/prototype/SummarizationModels/t5-base-model'\n",
        "t5_model = AutoModelForSeq2SeqLM.from_pretrained(drive_path_to_load_t5_model)\n",
        "\n",
        "# Load the T5 tokenizer\n",
        "drive_path_to_load_t5_tokenizer = 'gdrive/My Drive/prototype/SummarizationModels/t5-base-tokenizer'\n",
        "t5_tokenizer = AutoTokenizer.from_pretrained(drive_path_to_load_t5_tokenizer)\n",
        "\n",
        "# Load the BART model\n",
        "drive_path_to_load_bart_model = 'gdrive/My Drive/prototype/SummarizationModel2/facebook/bart-base-model'\n",
        "bart_model = AutoModelForSeq2SeqLM.from_pretrained(drive_path_to_load_bart_model)\n",
        "\n",
        "# Load the BART tokenizer\n",
        "drive_path_to_load_bart_tokenizer = 'gdrive/My Drive/prototype/SummarizationModel2/facebook/bart-base-tokenizer'\n",
        "bart_tokenizer = AutoTokenizer.from_pretrained(drive_path_to_load_bart_tokenizer)"
      ],
      "metadata": {
        "id": "A1PACriQwQXY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to capitalize sentences in the summary\n",
        "def capitalize_sentences(summary):\n",
        "    # Capitalize the first letter of each sentence\n",
        "    summary = re.sub(r'(?<=\\.\\s)(\\w)', lambda x: x.group(1).upper(), summary)\n",
        "\n",
        "    # Remove any leading punctuation or whitespace\n",
        "    summary = re.sub(r'^[^a-zA-Z]*', '', summary)\n",
        "\n",
        "    # Ensure the first character is uppercase\n",
        "    summary = summary[0].upper() + summary[1:]\n",
        "\n",
        "    # Add a full stop at the end if it's missing\n",
        "    if not summary.endswith('.'):\n",
        "        summary += '.'\n",
        "\n",
        "    return summary\n",
        "\n",
        "# Define a function to generate summaries using a given model and tokenizer\n",
        "def generate_summary(document, model, tokenizer):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    inputs = tokenizer.encode(document, return_tensors='pt', max_length=512, truncation=True)\n",
        "    inputs = inputs.to(device)\n",
        "\n",
        "    outputs = model.generate(inputs, max_length=250, min_length=80, length_penalty=1.5, num_beams=5, early_stopping=True, no_repeat_ngram_size=2)\n",
        "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return capitalize_sentences(summary)"
      ],
      "metadata": {
        "id": "Qz2z18tewVQc"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Document to be summarized\n",
        "document = \"\"\"DA is a widely accepted fruitful method to avoid overfitting issues and enhancing the performance of CNNs. However, currently, DA policies have been designed manually, and the optimal DA policies are dataset specific (Cubuk et al., 2019a; Khalifa et al., 2022). As a result, to design optimal DA policies manually for a given dataset requires a considerable amount of expertise in the DA domain, powerful computational resources, and a lot of time. So far, a significant focus of the researchers has been on refining the architectures of the CNNs to avoid data related issues. Less attention has been put into improving DA technologies, solving difficulties in traditional DA and automatically identifying optimal DA policies based on the dataset and task type (Cubuk et al., 2019a). The advancements in automated machine learning (AutoML) have shown promise in developing AutoDA systems to enhance CNN performance by automatically learning optimal DA policies (Yang et al., 2022a). However, limitations in existing AutoDA works highlight the need for further research to address the important problem of automating the DA process (Yang et al., 2022a).\"\"\"\n",
        "\n",
        "# Generate summaries using both models\n",
        "summary_t5 = generate_summary(document, t5_model, t5_tokenizer)\n",
        "summary_bart = generate_summary(document, bart_model, bart_tokenizer)\n"
      ],
      "metadata": {
        "id": "0deA7BG0wYdI"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the summaries\n",
        "print(\"Summary from T5 Model :\")\n",
        "print(summary_t5)\n",
        "print(\"\\nSummary from BART Model :\")\n",
        "print(summary_bart)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2_IQXzLwgCj",
        "outputId": "b4829bb1-4240-447f-992a-6f1df4cf8066"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary from T5 Model :\n",
            "The advancements in automated machine learning (AutoML) have shown promise in developing AutoDA systems to enhance CNN performance by automatically learning optimal DA policies (Yang et al., 2022a) however, limitations in existing autoDA works highlight the need for further research to address the important problem of automating the DB process (yang-etal. (2020a). As a result, to design optimal.\n",
            "\n",
            "Summary from BART Model :\n",
            "A widely accepted fruitful method to avoid overfitting issues and enhancing the performance of CNNs. However, currently, DA policies have been designed manually, and the optimal editor policies are dataset specific (Cubuk et al., 2019a; Khalifa etal., 2022). As a result, to design optimal DA policy manually for a given dataset requires a considerable amount of expertise in the DA domain, powerful computational resources, the lot of time. So far, a significant focus of the researchers has been on refining the architectures of wells to avoids data related issues. Less attention has being put into improving DA technologies, solving difficulties in traditional DA and automatically identifying optimal, policies based on the dataset and task type ( Cubuk ent al.)), The advancements in automated machine learning (AutoML) have shown promise in developing AutoDA systems to enhance CNN performance by automatically learning optimalDA policies (Yang et scissors, 2022a. However, limitations in existing Autoda works highlight the need for further research to address the important problem of automating the ED process ( Yang et y. 2021a).\n"
          ]
        }
      ]
    }
  ]
}